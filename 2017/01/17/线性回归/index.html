<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>逻辑回归总结 | 呜呜部落格(•̀ᴗ•́)و ̑̑</title>

  
  <meta name="author" content="Yang He">
  

  
  <meta name="description" content="总结了一下最近看的知识和笔记
线性回归
逻辑回归线性回归、岭回归、lasso回归和多项式回归模型。这些模型都是广义线性回归模型的具体形式，广义线性回归是一种灵活的框架，比普通线性回归要求更少的假设。似乎工业界中常用到逻辑回归，那就来看看广义线性回归模型的具体形式的另一种形式，逻辑回归（logisti">
  

  
  
  <meta name="keywords" content="">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="逻辑回归总结"/>

  <meta property="og:site_name" content="呜呜部落格(•̀ᴗ•́)و ̑̑"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="呜呜部落格(•̀ᴗ•́)و ̑̑" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">呜呜部落格(•̀ᴗ•́)و ̑̑</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>逻辑回归总结</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/01/17/线性回归/" rel="bookmark">
        <time class="entry-date published" datetime="2017-01-17T02:37:21.000Z">
          2017-01-17
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>总结了一下最近看的知识和笔记</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p><img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp5bva7dh9j313a0n8wg7.jpg" alt=""><br><img src="http://wx4.sinaimg.cn/mw690/006cxA6Hgy1fp5bvryhdcj313e0nrwn6.jpg" alt=""><br><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp5bvxa0qwj31360an0te.jpg" alt=""><br><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp5bvnh1loj312x0du3zm.jpg" alt=""></p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>线性回归、岭回归、lasso回归和多项式回归模型。这些模型都是广义线性回归模型的具体形式，广义线性回归是一种灵活的框架，比普通线性回归要求更少的假设。似乎工业界中常用到逻辑回归，那就来看看广义线性回归模型的具体形式的另一种形式，逻辑回归（logistic regression）。</p>
<p>据说LR简单、解释性好、计算速度快，和前面的模型不同，逻辑回归是用来做分类任务的。分类任务的目标是找一个函数，把观测值匹配到相关的类和标签上。学习算法必须用成对的特征向量和对应的标签来估计匹配函数的参数，从而实现更好的分类效果。简单来说， 逻辑回归是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。这里用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。</p>
<p>普通的线性回归假设响应变量呈正态分布，也称为高斯分布（Gaussian distribution ）或钟形曲线（bell curve）。正态分布数据是对称的，且均值，中位数和众数（mode）是一样的。很多自然现象都服从正态分布。如果响应变量不服从正态分布，就要用另外一种联连函数（解释变量到响应变量）了。所以说，逻辑回归假设因变量 y 服从<strong>伯努利分布</strong>，而线性回归假设因变量 y 服从<strong>高斯分布</strong>。</p>
<p>在逻辑回归里，响应变量描述了类似于掷一个硬币结果为正面的可能性。如果响应变量等于或超过了指定的临界值，预测结果就是正面，否则预测结果就是反面。响应变量是一个像线性回归中的解释变量构成的函数表示，称为逻辑函数（logistic function）。一个值在[0, 1]之间的逻辑函数如下所示<br><img src="http://wx1.sinaimg.cn/mw690/006cxA6Hgy1fp5cf5yah2j31dq0480st.jpg" alt=""><br>上式即为Sigmoid函数，是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0或者1。它的这个特性对于解决二分类问题十分重要。选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。</p>
<p>因此与线性回归有很多相同之处，去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。</p>
<p>其他基础知识就不做更多介绍，因为有很多资料，再讲几个我认为比较重要的点：</p>
<h3 id="逻辑回归的损失函数"><a href="#逻辑回归的损失函数" class="headerlink" title="逻辑回归的损失函数"></a>逻辑回归的损失函数</h3><p><code>什么是损失函数？</code></p>
<p>概况来讲，任何能够衡量模型预测出来的值与真实值之间的差异的函数都可以叫做代价函数 ,因此训练参数的过程就是不断改变θ，从而得到更小的损失函数的过程，在优化参数θ的过程中，最常用的方法是梯度下降。关于求解的优化方法以及代价函数的一些常见形式以后再开一篇仔细写写，现在先讲一下逻辑回归和线性回归的代价函数：<br>在线性回归中，最常用的是均方误差(Mean squared error)，即<br><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp5d5kgftxj31kw0bc769.jpg" alt=""><br>在逻辑回归中，最常用的是代价函数是交叉熵(Cross Entropy)，交叉熵是一个常见的代价函数，在神经网络中也会用到。下面是找到的《神经网络与深度学习》一书对交叉熵的解释：</p>
<p>交叉熵是对「出乎意料」的度量。神经元的目标是去计算函数x→y=y(x)。但是我们让它取而代之计算函数x→a=a(x)。假设我们把a当作y等于1的概率，1−a是y等于0的概率。那么，交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。香农信息量用来度量不确定性的大小：一个事件的香农信息量等于0，表示该事件的发生不会给我们提供任何新的信息，例如确定性的事件，发生的概率是1，发生了也不会引起任何惊讶；当不可能事件发生时，香农信息量为无穷大，这表示给我们提供了无穷多的新信息，并且使我们无限的惊讶。</p>
<p>根据交叉熵定义的损失函数如下：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp5d5odj9sj31kw04h3yy.jpg" alt=""></p>
<p>其实这个式子很好理解，即从式中可以看出， y=1 ，当预测值 h<em>\theta(x)=1  时，可以看出代价h</em>\theta(x)=\frac{1}{1+e^{-\theta^Tx}}函数 C(\theta) 的值为0，这正是我们希望的。如果预测值 h_\theta(x)=1 即 P(y=1|x;\theta)=0 ,意思是预测 y=1 的概率为0，但是事实上 y=1 ，因此代价函数 C(\theta)=\infty 相当于给学习算法一个惩罚。</p>
<h3 id="逻辑回归和朴素贝叶斯有什么区别？"><a href="#逻辑回归和朴素贝叶斯有什么区别？" class="headerlink" title="逻辑回归和朴素贝叶斯有什么区别？"></a>逻辑回归和朴素贝叶斯有什么区别？</h3><p><code>贝叶斯公式 + 条件独立假设 = 朴素贝叶斯方法</code></p>
<p>首先朴素贝叶斯里有一个很强的假设，就是条件独立假设。朴素贝叶斯作分类的时候，简单的说最终目的就是判断P（X=1）/P（X=0） &gt; 1 就好了，但是实际上除法并不好，容易产生过小的数值，发生underflow，所以我们两边同时取对数log函数。逻辑回归实际上是用线性回归模型的预测结果去逼近后验概率的逻辑发生比，能推导出朴素贝叶斯里有求和项，逻辑回归中也有求和项，并且表达式非常相似，但二者还是有区别的，用两种方法求出来的权重是不一样。产生差别的原因在于朴素贝叶斯方法的条件独立假设，因此，朴素贝叶斯可以不使用梯度下降，而直接通过统计每个特征的逻辑发生比来当权重，而逻辑回归的条件假设并不成立，通过梯度下降法可以得到特征之间的耦合信息，从而得到相应的权重。</p>
<p>此外，进一步说，朴素贝叶斯中的对数线性和Logistic回归中的对数线性作用不同。朴素贝叶斯中的log只是因为工程上的需要，而Logistic regression中的log可以有很多种解释。比方说从广义线性模型的角度解释的话，这个对数线性是因为lr把 P(y|x) 假设成一个二项分布（对于多分类是多项分布），而二项分布可以写成exponential family的标准形式。这也是LR的本质了，说到指数族分布,区别于指数分布，统计中很多熟悉的概率分布都是指数族分布的特定形式，（伯努利分布，高斯分布，多项分布），</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>


    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2018 Yang He
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>