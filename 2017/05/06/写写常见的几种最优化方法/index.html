<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>梯度下降法和牛顿法 | 呜呜部落格(•̀ᴗ•́)و ̑̑</title>

  
  <meta name="author" content="Yang He">
  

  
  <meta name="description" content="写写常见的几种最优化方法首先肯定是要说泰勒公式的了。。
泰勒公式是一个用函数在某点的信息描述其附近取值的公式。
局部有效性
它的基本形式是：
这是后续优化方法的迭代基础
梯度下降法在机器学习任务中，需要最小化损失函数L（theta），其中theta是要求解的模型参数。梯度下降法常用来求解这种无约束最">
  

  
  
  <meta name="keywords" content="">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="梯度下降法和牛顿法"/>

  <meta property="og:site_name" content="呜呜部落格(•̀ᴗ•́)و ̑̑"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="呜呜部落格(•̀ᴗ•́)و ̑̑" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">呜呜部落格(•̀ᴗ•́)و ̑̑</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">归档</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>梯度下降法和牛顿法</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/05/06/写写常见的几种最优化方法/" rel="bookmark">
        <time class="entry-date published" datetime="2017-05-06T08:35:00.000Z">
          2017-05-06
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="写写常见的几种最优化方法"><a href="#写写常见的几种最优化方法" class="headerlink" title="写写常见的几种最优化方法"></a>写写常见的几种最优化方法</h2><p>首先肯定是要说泰勒公式的了。。</p>
<p>泰勒公式是一个用函数在某点的信息描述其<em>附近</em>取值的公式。</p>
<p><strong>局部有效性</strong></p>
<p>它的基本形式是：<img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp5hmziysij31kw089tan.jpg" alt=""></p>
<p>这是后续优化方法的迭代基础</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>在机器学习任务中，需要最小化损失函数L（theta），其中theta是要求解的模型参数。梯度下降法常用来求解这种无约束最优化问题，它是一种迭代方法:选取初值，不断迭代，更新theta的值，进行损失函数的极小化。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。</p>
<p><img src="http://wx1.sinaimg.cn/mw690/006cxA6Hgy1fp5ht3pqwqj31kw0n8n2l.jpg" alt=""></p>
<p>这个图大概能表示梯度下降搜索迭代的概念</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Gradient_descent.png/350px-Gradient_descent.png" alt=""></p>
<p>同时还有共轭梯度下降，次梯度下降等。　共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p>
<p>还有两个概念：批量梯度下降法（Batch Gradient Descent，BGD），它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度会相当的慢。所以，这就引入了另外一种方法——随机梯度下降。随机梯度下降（Stochastic Gradient Descent，SGD）：随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>
<p>随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p>
<p>对批量梯度下降法和随机梯度下降法的总结：</p>
<p>   批量梯度下降—最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p>
<p>随机梯度下降—最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</p>
<p>所以引入了mini-batch 梯度下降的概念，很棒了。</p>
<h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>牛顿法和梯度下降法最直观的差别就是，前者进行了二阶的泰勒展开：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp5hxpzbrbj31kw0xnwn8.jpg" alt=""><br>一个动图<br><img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif" alt=""></p>
<p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p>
<p>但是！牛顿法每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</p>
<p>所以就有了拟牛顿法</p>
<h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><p>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
<p>这个又有很多内容了，DFP，BFGS啥的，在python里用起来很方便，但是最好还是要弄懂原理的，感觉心里会更清晰一点呢✌🏻</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>


    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2018 Yang He
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>