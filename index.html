<!DOCTYPE html>
<html >
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="Yang He" />


    
    


<meta property="og:type" content="website">
<meta property="og:title" content="呜呜部落格(•̀ᴗ•́)و ̑̑">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="呜呜部落格(•̀ᴗ•́)و ̑̑">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="呜呜部落格(•̀ᴗ•́)و ̑̑">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="呜呜部落格(•̀ᴗ•́)و ̑̑" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>呜呜部落格(•̀ᴗ•́)و ̑̑</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Yang He</a></h1>
        </hgroup>

        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">专注于前端</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Yang He</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Yang He</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:123@123.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-rnn seq" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/06/19/rnn seq/" class="article-date">
      <time datetime="2018-06-19T01:22:21.000Z" itemprop="datePublished">2018-06-19</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/19/rnn seq/">RNN seq2seq</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <p>之前对RNN的总结比较泛泛，今天从seq2seq这种具体模型来记录一下。之前是用LSTM做anomaly detection，可以看作一个一对一的模型，同时RNN（LSTM/GRU）在时间序列预测中也很常见，之前有些arima模型做预测，这里RNN可以看作本质上学的很好的ARIMA。</p>
<ul>
<li>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
        <p class="article-more-link">
          <a href="/2018/06/19/rnn seq/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post- CNN笔记(trick&amp;notes)" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/18/ CNN笔记(trick&notes)/" class="article-date">
      <time datetime="2018-05-18T01:22:21.000Z" itemprop="datePublished">2018-05-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/05/18/ CNN笔记(trick&notes)/">CNN笔记(trick&amp;notes)</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>杂七杂八的总结</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sigmoid会饱和，造成梯度消失。于是有了ReLU。</span><br><span class="line">ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。</span><br><span class="line">强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。</span><br><span class="line">太深了，梯度传不下去，于是有了highway。</span><br><span class="line">干脆连highway的参数都不要，直接变残差，于是有了ResNet。</span><br><span class="line">强行稳定参数的均值和方差，于是有了BatchNorm。</span><br><span class="line">在梯度流中增加噪声，于是有了 Dropout。</span><br><span class="line">RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。</span><br><span class="line">LSTM简化一下，有了GRU。</span><br><span class="line">GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。</span><br><span class="line">WGAN对梯度的clip有问题，于是有了WGAN-GP。</span><br></pre></td></tr></table></figure>
<blockquote>
<p>CNN中的网络设计应该逐渐减少图像尺寸，同时增加通道数，让空间信息转化为高阶抽象的特征信息。</p>
</blockquote>
<p>数据扩充方法包括：</p>
<p>1、翻转包括水平翻转、垂直翻转、水平垂直翻转。</p>
<p>2、旋转是常取的角度为-30、-15、15、30度等。</p>
<p>3、尺度变换是指将图像分辨率变为原图的0.8、1.1、1.2倍等，生成新图。</p>
<p>4、抠取包括随机抠取和监督式抠取。随机抠取在原图的随机位置抠取图像块作为新图像；监督式抠取只抠取含有明显语义信息的图像块。</p>
<p>5、色彩抖动是对原有的像素值分布进行轻微扰动，即加入轻微的噪声作为新图像。</p>
<p>6、Fancy PCA对所有训练数据的像素值进行主成分分析，根据得到的特征值和特征向量计算一组随机值，作为扰动加入到原来像素值中。</p>
<blockquote>
<p>局部最优解怎么办</p>
<p>模拟退火 加入momentum项</p>
</blockquote>
<p>不收敛怎么回事，怎么解决</p>
<p>数据太少<br>learningrate过大<br>可能导致从一开始就不收敛，每一层的w都很大，或者跑着跑着loss突然变得很大（一般是因为网络的前面使用relu作为激活函数而最后一层使用softmax作为分类的函数导致）<br>网络结构不好<br>更换其他的最优化算法<br>我做试验的时候遇到过一次，adam不收敛，用最简单的sgd收敛了。。具体原因不详<br>对参数做归一化<br>就是将输入归一化到均值为0方差为1然后使用BN等<br>初始化<br>改一种初始化的方案</p>
<blockquote>
<p>pooling层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。<br>max pooling比avg pooling效果会好一些。</p>
</blockquote>
<p>使用pretrain好的网络参数作为初始值。然后fine-tuning，此处可以保持前面层数的参数不变，只调节后面的参数。但是finetuning要考虑的是图片大小和跟原数据集的相关程度，如果相关性很高，那么只用取最后一层的输出，不相关数据多就要finetuning比较多的层。<br>初始值设置为0.1，然后训练到一定的阶段除以2，除以5，依次减小。<br>加入momentum项[2]，可以让网络更快的收敛。<br>节点数增加，learning rate要降低<br>层数增加，后面的层数learning rate要降低</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-迁移学习与多任务学习" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/04/07/迁移学习与多任务学习/" class="article-date">
      <time datetime="2018-04-07T15:35:21.000Z" itemprop="datePublished">2018-04-07</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/04/07/迁移学习与多任务学习/">迁移学习与多任务学习</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="迁移学习与多任务学习"><a href="#迁移学习与多任务学习" class="headerlink" title="迁移学习与多任务学习"></a>迁移学习与多任务学习</h2><p>最近考虑到adversarial machine learning的一些问题，涉及到train space 和 target space的变换问题，所以看了下迁移学习的内容，不过最近没有用这个，用了其他一些基于梯度的方法和GAN，由于没有用到估计很容易忘，所以记录一下。</p>
<p>首先介绍一下概念，再讲下区别，最后讲下可以用到的地方</p>
<h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>传统的机器学习需要对每个领域都标定大量训练数据，这将会耗费大量的人力与物力。而迁移学习（Transfer Learning）的目标是将从一个环境中学到的知识用来帮助新环境中的学习任务。因此，相对于传统的机器学习假设训练数据与测试数据服从相同的数据分布，迁移学习不会像传统机器学习那样作同分布假设。<br>迁移学习是指一个学习算法可以利用不同学习任务之间的共性来共享统计的优点和在任务间迁移知识。传统的机器学习假设训练数据与测试数据服从相同的数据分布。如果我们有了大量的、在不同分布下的训练数据，完全丢弃这些数据也是非常浪费的。如何合理的利用这些数据就是迁移学习主要解决的问题。迁移学习可以从现有的数据中迁移知识，用来帮助将来的学习。</p>
<blockquote>
<p>Transfer Learning (or Domain Adaptation): Giving a set of source domains/tasks t1, t2, …, t(n-1) and the target domain/task t(n), the goal is to learn well for t(n) by transferring some shared knowledge from t1, t2, …, t(n-1) to t(n). Although this definition is quite general, almost the entire literature on transfer learning is about supervised transfer learning and the number of source domains is only one (i.e., n=2). It also assumes that there are labeled training data for the source domain and few or no labeled examples in the target domain/task, but there are a large amount of unlabeled data in t(n). Note that the goal of transfer learning is to learn well only for the target task. Learning of the source task(s) is irrelevant.</p>
</blockquote>
<h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>目前多任务学习方法大致可以总结为两类，一是不同任务之间共享相同的参数（common parameter），二是挖掘不同任务之间隐藏的共有数据特征（latent feature）。</p>
<blockquote>
<p>Single Task Learning: Giving a set of learning tasks, t1 , t2 , …, t(n), learn each task independently. This is the most commonly used machine learning paradigm in practice.</p>
<p>Multitask Learning: Giving a set of learning tasks, t1 , t2 , …, t(n), co-learn all tasks simultaneously. In other words, the learner optimizes the learning/performance across all of the n tasks through some shared knowledge. This may also be called batch multitask learning. Online multitask learning is more like lifelong learning (see below).</p>
</blockquote>
<h3 id="两者区别"><a href="#两者区别" class="headerlink" title="两者区别"></a>两者区别</h3><p>迁移学习指从原任务获得一些transforming knowledge在目标任务重达到高精度。而多目标学习两者是同时进行的。这些问题都是由data  Domain Adaptation的问题引出的。</p>
<h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><ol>
<li><p>迁移学习<br>带 label 的 target data 很少，但是与 target data类似的 source data 很多的时候，迁移学习就可以将在 soure data 上训练好的网络用于 target task 上，如猫狗识别到老虎狮子识别。</p>
</li>
<li><p>多任务学习<br>使用未来预测现在；多种表示和度量；时间序列预测；使用不可操作特征<br>；使用额外任务来聚焦；<br>有序迁移；<br>多个任务自然地出现；<br>将输入变成输出；具体如脸部特征点检测， Fast R-CNN，旋转人脸网络</p>
</li>
<li><p>domain adversarial training</p>
</li>
</ol>
<p>挺有意思的，比如same task(手写数字识别)，data mismatch (MNIST, MNIST-M).<br>如果用 source data train 好一个网络后，直接应用于 target data 测试，效果不会很好，因为 source data 和 target data 的 distribution 相差太大。<br>Domain-adversarial 可以将不同 domain 的 data 转到同一个 domain 下，使它们具有相似的分布。</p>
<p>观察直接 train 出来的网络提取的特征，可以发现不同类的特征在 source data 上分离度较好，在 target data 上则较差。引入一个 domain classifier（相当于GAN 中 discriminator），用于判别 feature extractor 输出的 feature 属于 target domain 还是 source domain, feature extractor 则努力消除 features 分布在 target domain 和 source domain 上的差异，骗过domain classifier的鉴别。但是如果仅仅是使feature extractor 骗过domain classifier，只要feature extractor输出的 feature 全是零，就可以轻易做到的。所以我们需要：not only cheat the domain classifier ,but satisfying label classifier at the same time .</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-CNN" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/03/19/CNN/" class="article-date">
      <time datetime="2018-03-19T15:35:21.000Z" itemprop="datePublished">2018-03-19</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/19/CNN/">一些CNN知识</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>在做的实验室工作是工控系统中的对抗机器学习问题，这是一个从计算机视觉中扩展的问题，所以会遇到很多涉及到CNN的知识，这篇博客总结一下CNN的基础知识。</p>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><p>卷积神经网络是一种层次模型，其输入是原始数据，如图像、原始音频数据等。卷积神经网络通过卷<br>积操作、汇合操作和非线性激活函数映射等一系列操作的层层堆叠，将高层语义信息逐层由原<br>始数据输入层中抽取出来，逐层抽象，这一过程便是“前馈运算”。其中，不同类型操作在卷积神经网络中一般称作“层”:卷积操作对应“卷积层”，汇合操作对应“汇合层”等等。最终，卷积神经网络的最后一层将其目标任务(分类、回归等)形式化为目标函数。通过计算预测值与真实值之间的误差或损失，凭借反向传播算法将误差或损失由最后一层逐层向前反馈，更新每层参数。</p>
<blockquote>
<p>简单来讲，它就是一个搭积木的过程，将不同的层作为一个基本操作搭建在原始数据上，并用损失函数的计算作为结束。<br>很自然会想到和RNN的区别，CNN是在一个深度上纵向学习的过程，层数不断堆叠而RNN是引入时间信息后横向的学习过程。</p>
<p>用通俗的话来理解，就像是用一层层“滤镜”不断扫原始的图像，发现特征，最后再用一个全连接层进行最后的分类或者用卷积层替代完成。</p>
</blockquote>
<p><img src="http://wx4.sinaimg.cn/mw690/006cxA6Hgy1fsvlhtrnsqj30r80nqqd2.jpg" alt=""></p>
<h2 id="基本部件"><a href="#基本部件" class="headerlink" title="基本部件"></a>基本部件</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积是一种局部操作，通过一定大小的卷积核作用于局部图像区域获得图像的局部信息。之前信号处理学过卷积，它的物理意义可以理解成一个函数（如：单位响应）在另一个函数（如：输入信号）上的加权叠加（离散情况下容易理解）。之所以把卷积概念引入到神经网络中，是因为相比于之前的全连接层，需要大量的参数，原因在于每个神经元都和相邻层的神经元相连接，但是这种连接方式对于CV是不完全需要的。全连接层的方式对于图像数据来说似乎显得不这么友好，因为图像本身具有“二维空间特征”，通俗点说就是局部特性。所以如果我们可以用某种方式对一张图片的某个典型特征识别，那么这张图片的类别也就知道了，即引入卷积的概念。也就是说神经网络不再是对每个像素的输入信息做处理了,而是图片上每一小块像素区域进行处理, 这种做法加强了图片信息的连续性. 使得神经网络能看到图形, 而非一个点. 这种做法同时也加深了神经网络对图片的理解。</p>
<p><img src="http://wx2.sinaimg.cn/small/006cxA6Hgy1fsvp2grk1ug30em0aojsv.gif" alt=""></p>
<p>具体来说, 卷积神经网络有一个批量过滤器，filter, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理, 这时候整理出来的信息有了一些实际上的呈现, 比如这时的神经网络能看到一些边缘的图片信息, 然后在以同样的步骤, 用类似的批量过滤器（权值共享）扫过产生的这些边缘信息, 神经网络从这些边缘信息里面总结出更高层的信息结构,比如说总结的边缘能够画出眼睛,鼻子等等. 再经过一次过滤, 脸部的信息也从这些眼睛鼻子的信息中被总结出来. 最后我们再把这些信息套入几层普通的全连接神经层进行分类, 这样就能得到输入的图片能被分为哪一类的结果了.</p>
<p>同一层的神经元可以共享卷积核，那么对于高位数据的处理将会变得非常简单。并且使用卷积核后图片的尺寸变小，方便后续计算，并且我们不需要手动去选取特征，只用设计好卷积核的尺寸，数量和滑动的步长就可以让它自己去训练了，</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担 。 也就是说在卷积的时候, 我们不压缩长宽, 尽量地保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Max-Pooling: 选择Pooling窗口中的最大值作为采样值；</span><br><span class="line"></span><br><span class="line">Mean-Pooling: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值；</span><br></pre></td></tr></table></figure>
<p><img src="http://wx4.sinaimg.cn/small/006cxA6Hgy1fsvpv8hpimj31220lsju6.jpg" alt=""></p>
<p>汇合操作后的结果相比其输入降小了，其实汇合操 作实际上就是一种“降采样”操作</p>
<h3 id="激活函数和zero-padding"><a href="#激活函数和zero-padding" class="headerlink" title="激活函数和zero-padding"></a>激活函数和zero-padding</h3><p>激活函数层又称非线性映射层， 顾名思义，激活函数的引入为的是增加整个网络的表达能力(即非线性)。否则， 若干线性操作层的堆叠仍然只能起到线性映射的作用，无法形成复杂的函数。 在实际使用中，有多达十几种激活函数可供选择</p>
<p>所以到现在为止，我们的图片由4x4，通过卷积层变为3x3，再通过池化层变化2x2，如果我们再添加层，那么图片岂不是会越变越小？这个时候我们就会引出“Zero Padding”（补零），它可以帮助我们保证每次经过卷积或池化输出后图片的大小不变，如，上述例子我们如果加入Zero Padding，再采用3*3的卷积核，那么变换后的图片尺寸与原图片尺寸相同，如下图所示：</p>
<p><img src="http://wx2.sinaimg.cn/small/006cxA6Hgy1fsvq73zmn1j30pg0kcgms.jpg" alt=""></p>
<h3 id="总结一些中心思想"><a href="#总结一些中心思想" class="headerlink" title="总结一些中心思想"></a>总结一些中心思想</h3><ol>
<li>局部感受野：普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上；而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。比如1000×1000的图像，使用10×10的感受野，那么每个神经元只需要100个权值参数；不过由于需要将输入图像扫描一遍，共需要991×991个神经元（参数数目减少了一个数量级，不过还是太多）。</li>
<li>权值共享：在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。继续前面的例子，虽然需要991×991个神经元，但是它们的权值是共享的，所以只需要100个权值参数，以及1个偏置参数。作为补充，在CNN中的每个隐藏，一般会有多个卷积核。</li>
<li>池化：在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</li>
</ol>
<blockquote>
<p>训练过程也是前向计算和反向传播</p>
<p>一些问题：卷积核的参数如何确定？随机初始化一个数值后，是如何训练得到一个能够识别某些特征的卷积核的？如何调整CNN里的参数？如何设计最适合的CNN网络结构？对于这些问题，看完网上资料和文献后有了基本的理解。</p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-wuwu" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/02/27/wuwu/" class="article-date">
      <time datetime="2018-02-27T01:22:21.000Z" itemprop="datePublished">2018-02-27</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/27/wuwu/">LSTM &amp; RNN 回顾</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="LSTM-是什么"><a href="#LSTM-是什么" class="headerlink" title="LSTM 是什么"></a>LSTM 是什么</h2><p>最近在做的异常检测用到了LSTM，看了一些网上的介绍文章有的太复杂（全是公式），而且很多都是互相抄袭的，并没有说清楚LSTM的运行原理，那这里就按照我的理解解释一下好了：</p>
<p>本文将按照我理解的逻辑顺序依次介绍一些概念：首先是MLP</p>
<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p>先看图：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp6u2w5tl0j31400ma41y.jpg" alt=""></p>
<p>应该不需要再解释什么了，非常典型的多层感知机，选择合适的权重是会有不错的效果的。</p>
<p>以此为原型后续发展出了很多类型的神经网络，比如CNN等（CNN中就是把权重的乘号变成卷积）， 然后是RNN：</p>
<h3 id="RNN（Recurrent-Neural-Networks）"><a href="#RNN（Recurrent-Neural-Networks）" class="headerlink" title="RNN（Recurrent Neural Networks）"></a>RNN（Recurrent Neural Networks）</h3><p>RNN最大的不同点就是它引入了时序信息。一个简单的表示如下：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp6u4c09f3j30jq0ayjrt.jpg" alt=""></p>
<p>这个图可以说是非常平面了，意思是单个神经元上允许使用网络循环传递信息。这样的结构是与其他一般的神经网络是有不同的。</p>
<p>如果你还是不能理解他是怎么实现“循环”这个概念的，看下图：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/006cxA6Hgy1fp6u30oxtkj31400g6whk.jpg" alt=""></p>
<p>再仔细介绍一下这个图，因为rnn经常用的是文本处理，之类的场景，有的时候如果不好理解就代入到日常语境中会明白一些，比如这里，图上有4个T，可以理解的是如果一次处理一句话（batch的概念），这里一句话中分割成四个部分，并且进行tokenize, dictionarize,接着再由look up table 查找到embedding，将token由embedding表示，再对应到上图的输入神经元，隐状态 h_{i}^{t} 也就代表了一张MLP的hidden layer的一个cell。（全连接层，keras里的dense layer）</p>
<p>前向计算是明白的，解释一下反向传播，这里的反向传播叫BPTT（through time），RNN通过反向推理微调其权重来训练其单元。简单的说，就是根据单元计算出的总输出与目标输出之间的误差，从网络的最终输出端反向逐层回归，利用损失函数的偏导调整每个单元的权重。这就是著名的BP算法，而RNN网络使用的是类似的一个版本，称为通过时间的反向传播（BPTT）。该版本扩展了调整过程，包括负责前一时刻（T-1）输入值对应的每个单元的记忆的权重。</p>
<p><img src="http://wx4.sinaimg.cn/mw690/006cxA6Hgy1fp6u34ny2cj31kw0lrn16.jpg" alt=""></p>
<p>st=Uh（t−1）+Wxt</p>
<p>ht=f(st)</p>
<p>zt=Vht</p>
<p>ŷ t=softmax(zt)</p>
<p>（省略偏置项）</p>
<p>我们要迭代更新的是U，V，W矩阵，三个权重矩阵在时间维度上是共享的。这可以理解为：每个时刻都在执行相同的任务，所以是共享的。这个时候损失函数就是所有时间的损失和。然后就是对U，V，W求梯度。</p>
<p>这里就引入了记忆的信息。但是，RNN会出现Gradient Vanish。先介绍一下梯度消失的概念：在多层网络中，影响梯度大小的因素主要有两个：权重和激活函数的偏导。深层的梯度是多个激活函数偏导乘积的形式来计算，如果这些激活函数的偏导比较小（小于1）或者为0，那么梯度随时间很容易vanishing；相反，如果这些激活函数的偏导比较大（大于1），那么梯度很有可能就会exploding。直观上讲，使用tanh或logistic激活函数时，由于导数值分别在0到1之间、0到1/4之间，所以如果权重矩阵 U 的范数也不很大，那么经过 t−k 次传播后，值会趋于0，也就导致了梯度消失问题。因而，梯度的计算和更新非常困难。关于梯度消失和激活函数见上一篇笔记。为了缓解梯度消失，可以使用ReLU、PReLU来作为激活函数，以及将 U 初始化为单位矩阵（而不是用随机初始化）等方式。</p>
<p>下图直观的表达了梯度消失：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp6u39xbvjj31400lead4.jpg" alt=""></p>
<p>为了减小梯度消失带来的影响，LSTM出现了：</p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>有些场景下，由于传递的信息路径太长了，RNN处理的效果不好，这时候LSTM就派上用场了。<br>LSTM（Long Short Term Memory networks）是一种特殊的RNN，它能学习和存储长期依赖关系。它具有循环神经网络的特性：重复的链式网络存储结构。</p>
<p>LSTM比较核心的概念就是门了，看这个图：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp6u3rp9mrj312c0h4di3.jpg" alt=""></p>
<p>这个图看着比较舒服，也能比较直观的表示它运行的原理。LSTM单元引入了门机制（Gate），通过遗忘门、输入门和输出门来控制流过单元的信息。我们知道，Simple RNN之所以有梯度消失是因为误差项之间的相乘关系；如果用LSTM推导，会发现这个相乘关系变成了相加关系，所以可以缓解梯度消失。</p>
<p>先讲LSTM的前向计算，再讲它的误差反向传播。</p>
<blockquote>
<p>遗忘门（forget gate）<br>它决定了上一时刻的单元状态 c_t-1 有多少保留到当前时刻 c_t</p>
<p>输入门（input gate）<br>  它决定了当前时刻网络的输入 x_t 有多少保存到单元状态 c_t</p>
<p>输出门（output gate）<br>控制单元状态 c_t 有多少输出到 LSTM 的当前输出值 h_t</p>
</blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-c9dbab3979794684.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/243" alt=""></p>
<p>这个c就是比RNN增加的状态，表示长期状态。上面说的这三个门就是控制这个长期状态，就是gate。其实gate也是一层全连接层，输入是一个向量，输出是一个 0到1 之间的实数向量。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-bb8d1d0c2c5aa0f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/186" alt=""></p>
<p>为什么这个能进行控制呢，因为门的输出是 0到1 之间的实数向量，当门输出为 0 时，任何向量与之相乘都会得到 0 向量，这就相当于什么都不能通过；输出为 1 时，任何向量与之相乘都不会有任何改变，这就相当于什么都可以通过。</p>
<p>再仔细讲一下三个门的原理，</p>
<p>遗忘门：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-81bc580ebc1b51e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/666" alt=""></p>
<p>Wf遗忘门权重。</p>
<p>输入门：</p>
<p>根据上一次的输出和本次输入来计算当前输入的单元状态：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-f013288618c83b31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/620" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-6443c16af1fc9fa1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/568" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-9addd095ca99f567.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/611" alt=""></p>
<p>输出门：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-963ff8645885f284.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></p>
<p>这几张图能比较清晰的表现出LSTM的运行原理的。</p>
<p>总的就是这个了：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-e7209fdb040ea1da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/482" alt=""></p>
<p>再讲反向传播：<br>反向计算每个神经元的误差项值。与 RNN 一样，LSTM 误差项的反向传播也是包括两个方向：<br>一个是沿时间的反向传播，即从当前 t 时刻开始，计算每个时刻的误差项；<br>一个是将误差项向上一层传播。 根据相应的误差项，计算每个权重的梯度。权重矩阵 W 都是由两个矩阵拼接而成，这两部分在反向传播中使用不同的公式，因此在后续的推导中，权重矩阵也要被写为分开的两个矩阵。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-9da34d2b2b475e7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></p>
<p>Wx是层间的，Wh是时间上的</p>
<h3 id="再讲一下LSTM为什么能解决RNN不能解决的梯度消失"><a href="#再讲一下LSTM为什么能解决RNN不能解决的梯度消失" class="headerlink" title="再讲一下LSTM为什么能解决RNN不能解决的梯度消失"></a>再讲一下LSTM为什么能解决RNN不能解决的梯度消失</h3><p>LSTM使用gate function，有选择的让一部分信息通过。gate是由一个sigmoid单元和一个逐点乘积操作组成，sigmoid单元输出1或0，用来判断通过还是阻止，然后训练这些gate的组合。所以，当gate是打开的（梯度接近于1），梯度就不会vanish。并且sigmoid不超过1，那么梯度也不会explode。</p>
<p>两个点：</p>
<p>1、当gate是关闭的，那么就会阻止对当前信息的改变，这样以前的依赖信息就会被学到。2、当gate是打开的时候，并不是完全替换之前的信息，而是在之前信息和现在信息之间做加权平均。所以，无论网络的深度有多深，输入序列有多长，只要gate是打开的，网络都会记住这些信息。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-gaga" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/01/31/gaga/" class="article-date">
      <time datetime="2018-01-31T01:22:21.000Z" itemprop="datePublished">2018-01-31</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/31/gaga/">激活函数和梯度消失问题</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="梯度消失和激活函数"><a href="#梯度消失和激活函数" class="headerlink" title="梯度消失和激活函数"></a>梯度消失和激活函数</h2><p>本文简单介绍下常见的激活函数和梯度消失的概念：</p>
<p>激活函数：激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。正因为上面的原因，需要引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释balabala）。</p>
<h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14856939826808.png" alt=""></p>
<p>sigmoid及一阶导：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6unuu8ojj31kw0j4q5y.jpg" alt=""></p>
<p>sigmoid函数常用来做二分类。同时带来很多缺点：激活函数计算量大（指数运算），反向传播求误差梯度时，求导涉及除法<br>对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），从而无法完成深层网络的训练。</p>
<p>仔细讲一下梯度消失：</p>
<p>根本的问题其实并非是梯度消失问题或者梯度激增问题，而是在前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不稳定了。简而言之，真实的问题就是神经网络受限于不稳定梯度的问题。所以，如果我们使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。</p>
<h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14856939707785.png" alt=""></p>
<p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14856940084631.png" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6unysn5dj31kw0ojdiz.jpg" alt=""><br>也有梯度消失的问题，但是跟sigmoid不一样的是它是关于零点中心对称的。作为非中心对称的激活函数，sigmoid有个问题：输出总是正数，那么其对wi权重的导数总是正数或负数，这会导致可能会产生阶梯式更新，这显然并非一个好的优化路径。</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14854155167846.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6uo2uo8yj31kw0hzmzk.jpg" alt=""></p>
<p>ReLU作为目前用的最多的激活函数，有很多的优点，比如解决了梯度消失的问题 (在正区间)<br>计算速度非常快，只需要判断输入是否大于0，收敛速度远快于sigmoid和tanh，更容易学习优化。因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的sigmoid函数，由于两端饱和，在传播过程中容易丢弃信息但是也存在一些问题，如上图，它也不是关于中心对称的，并且可能存在Dead ReLU Problem，意思是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) 学习速率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0。解决方法是可以采用Xavier初始化方法，以及避免将学习速率设置太大或使用adagrad等自动调节学习速率的算法。这时候就有了ELU。</p>
<p><a href="http://blog.csdn.net/disiwei1012/article/details/79204243" target="_blank" rel="noopener">这篇博客介绍了dead的原因</a></p>
<p>此外，有实验说，大概 80%-90%的特征都会被截断，然而 ReLU 仍然是非常常用的激励函数，因为特征足够多。</p>
<h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6uo6iqikj31kw0lego9.jpg" alt=""></p>
<p>不会有Dead ReLU问题<br>输出的均值接近0，zero-centered</p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p><img src="https://upload-images.jianshu.io/upload_images/1667471-5bf75eefed2154f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/643" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-c798481b7b366cb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/203" alt=""></p>
<p>就是如果某一个 zj 大过其他 z, 那这个映射的分量就逼近于 1,其他就逼近于 0，主要应用就是多分类。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-未命名" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/01/12/未命名/" class="article-date">
      <time datetime="2018-01-12T01:22:21.000Z" itemprop="datePublished">2018-01-12</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/12/未命名/">一个有意思的Deep Learning笔记</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <iframe src=" /pdf/introdl.pdf" style="width:750px; height:500px;" frameborder="0"></iframe>
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-XGBoost原理" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/12/02/XGBoost原理/" class="article-date">
      <time datetime="2017-12-02T02:22:21.000Z" itemprop="datePublished">2017-12-02</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/02/XGBoost原理/">集成算法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="RF、GBDT和xgboost"><a href="#RF、GBDT和xgboost" class="headerlink" title="RF、GBDT和xgboost"></a>RF、GBDT和xgboost</h2><h3 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h3><p>bagging很好理解的。从M个训练样本中随机选取m个样本，从N个特征中随机选取n个特征，然后建立一颗决策树。这样训练出T棵树后，让这k颗树对测试集进行投票产生决策值。RF是一种bagging的思路。可以并行化处理。</p>
<p>随机森林的优点较多，简单总结：1、在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；2、能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；3、容易做成并行化方法。<br>RF的缺点：在噪声较大的分类或者回归问题上回过拟合。</p>
<p>我用随机森林做异常检测的效果还是不错的。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>总共构建T棵树。当构建到第t棵树的时候，需要对前t-1棵树对训练样本分类回归产生的残差进行拟合。每次构建树的方式以及数据集一样，只不过拟合的目标变成了t-1棵树输出的残差。不可并行化处理。从CART来的，GBDT是回归树，当然也能分类，其实做分类的时候和Adaboost有点像，就是给个权重。GBDT中的决策树是个弱模型，深度较小一般不会超过5，叶子节点的数量也不会超过10，对于生成的每棵决策树乘上比较小的缩减系数（学习率&lt;0.1），有些GBDT的实现加入了随机抽样（subsample 0.5&lt;=f &lt;=0.8）提高模型的泛化能力。通过交叉验证的方法选择最优的参数。</p>
<p>GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。<br>在GradientBoosting算法中，关键就是利用<strong>损失函数的负梯度方向在当前模型的值作为残差的近似值</strong>，进而拟合一棵CART回归树。<br>GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，</p>
<blockquote>
<p>类比梯度下降会很好理解</p>
</blockquote>
<p>GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显，1、它能灵活的处理各种类型的数据；2、在相对较少的调参时间下，预测的准确度较高。<br>当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。</p>
<h3 id="XGBOOST"><a href="#XGBOOST" class="headerlink" title="XGBOOST"></a>XGBOOST</h3><p>总共构建T颗树。当构建到第t颗树的时候，需要对前t-1颗树对训练样本分类回归产生的残差进行拟合。每次拟合产生新的树的时候，遍历所有可能的树，并选择使得目标函数值（cost）最小的树。但是这样在实践中难以实现，因此需要将步骤进行分解，在构造新的树的时候，每次只产生一个分支，并选择最好的那个分支。如果产生分支的目标函数值（cost）比不产生的时候大或者改进效果不明显，那么就放弃产生分支（相当于truncate，截断）。可以并行化处理，效率比GBDT高，效果比GBDT好。</p>
<h3 id="区别和联系"><a href="#区别和联系" class="headerlink" title="区别和联系"></a>区别和联系</h3><p>Xgboost是GB算法的高效实现，xgboost中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）。 </p>
<pre><code>(1). xgboost在目标函数中显示的加上了正则化项，基学习为CART时，正则化项与树的叶子节点的数量T和叶子节点的值有关。 
(2). GB中使用Loss Function对f(x)的一阶导数计算出伪残差用于学习生成fm(x)，xgboost不仅使用到了一阶导数，还使用二阶导数。 
(3). 上面提到CART回归树中寻找最佳分割点的衡量标准是最小化均方差，xgboost寻找分割点的标准是最大化，lamda，gama与正则化项相关。
</code></pre><p>有一些比较难理解的点：</p>
<blockquote>
<p>1、在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。<br>2、xgboost<strong>考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率</strong>，paper提到50倍。<br>特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。<br>按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。<br>3、xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。</p>
</blockquote>
<p>关于GBDT和XGboost还是蛮简单的：</p>
<p>GBDT和随机森林的相同点：<br>1、都是由多棵树组成<br>2、最终的结果都是由多棵树一起决定</p>
<p><code>GBDT和随机森林的不同点：</code> </p>
<p>1、组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成<br>2、组成随机森林的树可以并行生成；而GBDT只能是串行生成<br>3、对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来<br>4、随机森林对异常值不敏感，GBDT对异常值非常敏感<br>5、随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成<br>6、随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能</p>
<p><code>GBDT和XGBoost区别</code></p>
<p>1、传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）。 </p>
<p>2、传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数。 </p>
<p>3、XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，放置过拟合，这也是XGBoost优于传统GBDT的一个特性。 </p>
<p>4、Shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）。 </p>
<p>5、列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算。</p>
<p>6、对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动 学习出它的分裂方向。 </p>
<p>7、XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
<h2 id="XGBoost原理"><a href="#XGBoost原理" class="headerlink" title="XGBoost原理"></a>XGBoost原理</h2><p>GBDT在函数空间中利用梯度下降法进行优化，XGBoost在函数空间中用牛顿法进行优化。</p>
<p>实际上GBDT泛指所有梯度提升树算法，包括XGBoost，它也是GBDT的 一种变种，这里为了区分它们，GBDT特指“Greedy Function Approximation:A Gradient Boosting Machine”里提出的算法，它只用了 一阶导数信息。</p>
<p><a href="http://djjowfy.com/2017/08/01/XGBoost的原理/" target="_blank" rel="noopener">一个写的蛮好的博客链接</a></p>
<p>不总结啦。</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-ARIMA😛" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/10/08/ARIMA😛/" class="article-date">
      <time datetime="2017-10-08T01:22:21.000Z" itemprop="datePublished">2017-10-08</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/10/08/ARIMA😛/">利用ARIMA模型做时间序列预测</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="ARIMA😛"><a href="#ARIMA😛" class="headerlink" title="ARIMA😛"></a>ARIMA😛</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p>时间序列，就是按时间顺序排列的，随时间变化的数据序列。我从自己实验平台上采的上位机和设备之间的通信包，按packets/s做数据就是时间序列。因为这个环境比较特殊，跟通信过程和应用层协议有关系，这里就不多说了，因为不是传统网络包，所以前期还是做了一些分析的，结果是这些数据是周期性的，但是不平稳，总之是可以做很多东西的。</p>
<p>随机过程的特征有均值、方差、协方差等。<br>如果随机过程的特征随着时间变化，则此过程是非平稳的；相反，如果随机过程的特征不随时间而变化，就称此过程是平稳的。非平稳时间序列分析时，若导致非平稳的原因是确定的，可以用的方法主要有趋势拟合模型、季节调整模型、移动平均、指数平滑等方法。<br>若导致非平稳的原因是随机的，方法主要有ARIMA（autoregressive integrated moving average）及自回归条件异方差模型等。</p>
<p>ARIMA，就是AR，I，MA的结合。ARIMA(p,d,q)模型，其中 d 是差分的阶数，用来得到平稳序列。</p>
<p>AR是自回归, p为相应的自回归项。MA为移动平均，q为相应的移动平均项数。ARIMA和ARMA的区别，就是公式左边的x变成差分算子，保证数据的稳定性。一般取两阶以下就好了。</p>
<p>贴公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-e3570dff8b96abda.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/515" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-994e75bd9e86802c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/517" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-9fefb203841154f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/617" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-80aa172908765c87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/488" alt=""></p>
<h3 id="上手"><a href="#上手" class="headerlink" title="上手"></a>上手</h3><p>ARIMA模型运用的基本流程有几下几步：</p>
<pre><code>数据可视化，识别平稳性。
对非平稳的时间序列数据，做差分，得到平稳序列。
建立合适的模型。
平稳化处理后，若偏自相关函数是截尾的，而自相关函数是拖尾的，则建立AR模型；
若偏自相关函数是拖尾的，而自相关函数是截尾的，则建立MA模型；
若偏自相关函数和自相关函数均是拖尾的，则序列适合ARMA模型。
模型的阶数在确定之后，对ARMA模型进行参数估计，比较常用是最小二乘法进行参数估计。
假设检验，判断（诊断）残差序列是否为白噪声序列。
利用已通过检验的模型进行预测。
</code></pre><p>1.数据的平稳性处理<br>ARIMA模型建模时，首先采用ADF（Augmented Dickey-Fuller）单位根检验来判断数据的平稳性。通常可以画出时间序列的散点图或折线图，来对所研究的时间序列进行大致的平稳性判断。对非平稳的时间序列，一般取对数处理或进行差分处理，然后判断修正后的数据序列的平稳性。若采取差分的形式，此时进行差分的次数就是ARIMA(p,d,q)模型中的阶数d。在差分运算过程中，阶数并不是越大越好，差分运算的过程是信息加工提取的过程，因此，一般差分次数不超过2次。时间序列数据被平稳化处理后，ARIMA(p,d,q)模型就转化为ARMA(p,q)模型。</p>
<p>2.建立模型<br>通常在时间序列分析中，采用自相关函数（ACF）、偏自相关函数（PACF）来判别ARMA(p,q)模型的系数和阶数。自相关函数(ACF)描述时间序列观测值与其过去的观测值之间的线性相关性。偏自相关函数(PACF)描述在给定中间观测值的条件下时间序列观测值与其过去的观测值之间的线性相关性。<br><img src="http://img.blog.csdn.net/20150602093858686?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXNwaXJpbnZhZ3JhbnQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>这个表基本上就够用啦</p>
<p>3.参数估计<br>时间序列分析模型的阶数在确定之后，然后应当对ARMA模型进行参数估计。比较常用是最小二乘法进行参数估计，但是在所有的时间序列模型中，MA模型的参数估计相对比较困难，因此，尽量避免使用高阶的MA模型和ARMA模型。</p>
<p>4.模型验证<br>通过上述步骤后，应对通过模型取得的估计结果进行检验与诊断，以验证所选用的模型是否合适。这一过程主要检验所拟合的时间序列模型是否客观合理。针对模型的合理性检验，通常从两个方面进行判断：1、要验证所拟合的时间序列<br>模型的参数估计值是否有显著性；2、要验证所拟合的时间序列模型的残差序列是否是白噪声序列，即残差序列的独立性检验。残差序列可由估计出来的模型计算得到，如果残差序列的自相关函数不显著非零，可以认为是独立的。若这两项验证通过，则认为该模型是合理的，否则，应重新选取模型，上述步骤，选出有效的模型，然后应用该模型进行预测。</p>
<p>这里不把我对工控数据的arima分析贴出来了，其实比较一下仔细分析选的模型和autoarima的特性还是蛮有趣的</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-git账号" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2017/08/02/git账号/" class="article-date">
      <time datetime="2017-08-02T01:22:21.000Z" itemprop="datePublished">2017-08-02</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/08/02/git账号/">查看本地git的账号是否是目前登陆的邮箱</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>我的GitHub账号很乱。感觉</p>
<p>我查了一下，很多人出现跟我类似的问题，都在问为什么自己明明有干嘛干嘛啊，为什么没有绿格子，基本上解决办法就是本地看一下git账号邮箱啊用户名啊是什么就能解决了。</p>
<p><code>$ git config --global user.name &quot;John Doe&quot;</code></p>
<p><code>$ git config --global user.email johndoe@example.com</code></p>
<p>这个是一开始的配置，可能大家配置的时候很不经意 就填了其他的。。像我这种有好几个邮箱的人很多吧。这个其实蛮关键的，因为以后你所有的项目都会默认使用这里配置的用户信息。</p>
<p>为什么没有绿格子呢，看一下配置信息就好啦</p>
<p><code>$ git config --list</code></p>
<p>不是自己登陆的邮箱账号的话就改吧</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2016-2018 Yang He
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>