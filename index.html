<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>呜呜部落格(•̀ᴗ•́)و ̑̑</title>

  
  <meta name="author" content="Yang He">
  

  

  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  

  <meta property="og:site_name" content="呜呜部落格(•̀ᴗ•́)و ̑̑"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="呜呜部落格(•̀ᴗ•́)و ̑̑" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">呜呜部落格(•̀ᴗ•́)و ̑̑</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives">归档</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    
  <article>

  
    
    <h3 class="article-title"><a href="/2018/03/09/wuwu/"><span></span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/03/09/wuwu/" rel="bookmark">
        <time class="entry-date published" datetime="2018-03-09T11:13:40.537Z">
          2018-03-09
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id=""><a href="#" class="headerlink" title=" "></a>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2018/01/31/gaga/"><span>激活函数和梯度消失问题</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/31/gaga/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-31T01:22:21.000Z">
          2018-01-31
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="梯度消失和激活函数"><a href="#梯度消失和激活函数" class="headerlink" title="梯度消失和激活函数"></a>梯度消失和激活函数</h2><p>本文简单介绍下常见的激活函数和梯度消失的概念：</p>
<p>激活函数：激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。正因为上面的原因，需要引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释balabala）。</p>
<h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14856939826808.png" alt=""></p>
<p>sigmoid及一阶导：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6unuu8ojj31kw0j4q5y.jpg" alt=""></p>
<p>sigmoid函数常用来做二分类。同时带来很多缺点：激活函数计算量大（指数运算），反向传播求误差梯度时，求导涉及除法<br>对于深层网络，sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在 sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），从而无法完成深层网络的训练。</p>
<p>仔细讲一下梯度消失：</p>
<p>根本的问题其实并非是梯度消失问题或者梯度激增问题，而是在前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不稳定了。简而言之，真实的问题就是神经网络受限于不稳定梯度的问题。所以，如果我们使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。</p>
<h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14856939707785.png" alt=""></p>
<p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14856940084631.png" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6unysn5dj31kw0ojdiz.jpg" alt=""><br>也有梯度消失的问题，但是跟sigmoid不一样的是它是关于零点中心对称的。作为非中心对称的激活函数，sigmoid有个问题：输出总是正数，那么其对wi权重的导数总是正数或负数，这会导致可能会产生阶梯式更新，这显然并非一个好的优化路径。</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p><img src="https://feisky.xyz/machine-learning/neural-networks/images/14854155167846.jpg" alt=""></p>
<p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6uo2uo8yj31kw0hzmzk.jpg" alt=""></p>
<p>ReLU作为目前用的最多的激活函数，有很多的优点，比如解决了梯度消失的问题 (在正区间)<br>计算速度非常快，只需要判断输入是否大于0，收敛速度远快于sigmoid和tanh，更容易学习优化。因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的sigmoid函数，由于两端饱和，在传播过程中容易丢弃信息但是也存在一些问题，如上图，它也不是关于中心对称的，并且可能存在Dead ReLU Problem，意思是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) 学习速率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0。解决方法是可以采用Xavier初始化方法，以及避免将学习速率设置太大或使用adagrad等自动调节学习速率的算法。这时候就有了ELU。</p>
<p><a href="http://blog.csdn.net/disiwei1012/article/details/79204243" target="_blank" rel="noopener">这篇博客介绍了dead的原因</a></p>
<p>此外，有实验说，大概 80%-90%的特征都会被截断，然而 ReLU 仍然是非常常用的激励函数，因为特征足够多。</p>
<h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><p><img src="http://wx3.sinaimg.cn/mw690/006cxA6Hgy1fp6uo6iqikj31kw0lego9.jpg" alt=""></p>
<p>不会有Dead ReLU问题<br>输出的均值接近0，zero-centered</p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p><img src="https://upload-images.jianshu.io/upload_images/1667471-5bf75eefed2154f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/643" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-c798481b7b366cb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/203" alt=""></p>
<p>就是如果某一个 zj 大过其他 z, 那这个映射的分量就逼近于 1,其他就逼近于 0，主要应用就是多分类。</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2018/01/12/未命名/"><span>一个有意思的Deep Learning笔记</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/01/12/未命名/" rel="bookmark">
        <time class="entry-date published" datetime="2018-01-12T01:22:21.000Z">
          2018-01-12
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <iframe src=" /pdf/introdl.pdf" style="width:750px; height:500px;" frameborder="0"></iframe>
      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2017/12/02/XGBoost原理/"><span>集成算法</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/12/02/XGBoost原理/" rel="bookmark">
        <time class="entry-date published" datetime="2017-12-02T02:22:21.000Z">
          2017-12-02
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="RF、GBDT和xgboost"><a href="#RF、GBDT和xgboost" class="headerlink" title="RF、GBDT和xgboost"></a>RF、GBDT和xgboost</h2><h3 id="RF"><a href="#RF" class="headerlink" title="RF"></a>RF</h3><p>bagging很好理解的。从M个训练样本中随机选取m个样本，从N个特征中随机选取n个特征，然后建立一颗决策树。这样训练出T棵树后，让这k颗树对测试集进行投票产生决策值。RF是一种bagging的思路。可以并行化处理。</p>
<p>随机森林的优点较多，简单总结：1、在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；2、能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；3、容易做成并行化方法。<br>RF的缺点：在噪声较大的分类或者回归问题上回过拟合。</p>
<p>我用随机森林做异常检测的效果还是不错的。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>总共构建T棵树。当构建到第t棵树的时候，需要对前t-1棵树对训练样本分类回归产生的残差进行拟合。每次构建树的方式以及数据集一样，只不过拟合的目标变成了t-1棵树输出的残差。不可并行化处理。从CART来的，GBDT是回归树，当然也能分类，其实做分类的时候和Adaboost有点像，就是给个权重。GBDT中的决策树是个弱模型，深度较小一般不会超过5，叶子节点的数量也不会超过10，对于生成的每棵决策树乘上比较小的缩减系数（学习率&lt;0.1），有些GBDT的实现加入了随机抽样（subsample 0.5&lt;=f &lt;=0.8）提高模型的泛化能力。通过交叉验证的方法选择最优的参数。</p>
<p>GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。<br>在GradientBoosting算法中，关键就是利用<strong>损失函数的负梯度方向在当前模型的值作为残差的近似值</strong>，进而拟合一棵CART回归树。<br>GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，</p>
<blockquote>
<p>类比梯度下降会很好理解</p>
</blockquote>
<p>GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显，1、它能灵活的处理各种类型的数据；2、在相对较少的调参时间下，预测的准确度较高。<br>当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据。</p>
<h3 id="XGBOOST"><a href="#XGBOOST" class="headerlink" title="XGBOOST"></a>XGBOOST</h3><p>总共构建T颗树。当构建到第t颗树的时候，需要对前t-1颗树对训练样本分类回归产生的残差进行拟合。每次拟合产生新的树的时候，遍历所有可能的树，并选择使得目标函数值（cost）最小的树。但是这样在实践中难以实现，因此需要将步骤进行分解，在构造新的树的时候，每次只产生一个分支，并选择最好的那个分支。如果产生分支的目标函数值（cost）比不产生的时候大或者改进效果不明显，那么就放弃产生分支（相当于truncate，截断）。可以并行化处理，效率比GBDT高，效果比GBDT好。</p>
<h3 id="区别和联系"><a href="#区别和联系" class="headerlink" title="区别和联系"></a>区别和联系</h3><p>Xgboost是GB算法的高效实现，xgboost中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）。 </p>
<pre><code>(1). xgboost在目标函数中显示的加上了正则化项，基学习为CART时，正则化项与树的叶子节点的数量T和叶子节点的值有关。 
(2). GB中使用Loss Function对f(x)的一阶导数计算出伪残差用于学习生成fm(x)，xgboost不仅使用到了一阶导数，还使用二阶导数。 
(3). 上面提到CART回归树中寻找最佳分割点的衡量标准是最小化均方差，xgboost寻找分割点的标准是最大化，lamda，gama与正则化项相关。
</code></pre><p>有一些比较难理解的点：</p>
<blockquote>
<p>1、在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。<br>2、xgboost<strong>考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率</strong>，paper提到50倍。<br>特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。<br>按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。<br>3、xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。</p>
</blockquote>
<p>关于GBDT和XGboost还是蛮简单的：</p>
<p>GBDT和随机森林的相同点：<br>1、都是由多棵树组成<br>2、最终的结果都是由多棵树一起决定</p>
<p><code>GBDT和随机森林的不同点：</code> </p>
<p>1、组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成<br>2、组成随机森林的树可以并行生成；而GBDT只能是串行生成<br>3、对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来<br>4、随机森林对异常值不敏感，GBDT对异常值非常敏感<br>5、随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成<br>6、随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能</p>
<p><code>GBDT和XGBoost区别</code></p>
<p>1、传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）。 </p>
<p>2、传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数。 </p>
<p>3、XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，放置过拟合，这也是XGBoost优于传统GBDT的一个特性。 </p>
<p>4、Shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）。 </p>
<p>5、列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算。</p>
<p>6、对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动 学习出它的分裂方向。 </p>
<p>7、XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
<h2 id="XGBoost原理"><a href="#XGBoost原理" class="headerlink" title="XGBoost原理"></a>XGBoost原理</h2><p>GBDT在函数空间中利用梯度下降法进行优化，XGBoost在函数空间中用牛顿法进行优化。</p>
<p>实际上GBDT泛指所有梯度提升树算法，包括XGBoost，它也是GBDT的 一种变种，这里为了区分它们，GBDT特指“Greedy Function Approximation:A Gradient Boosting Machine”里提出的算法，它只用了 一阶导数信息。</p>
<p><a href="http://djjowfy.com/2017/08/01/XGBoost的原理/" target="_blank" rel="noopener">一个写的蛮好的博客链接</a></p>
<p>不总结啦。</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2017/10/08/ARIMA😛/"><span>利用ARIMA模型做时间序列预测</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/10/08/ARIMA😛/" rel="bookmark">
        <time class="entry-date published" datetime="2017-10-08T01:22:21.000Z">
          2017-10-08
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="ARIMA😛"><a href="#ARIMA😛" class="headerlink" title="ARIMA😛"></a>ARIMA😛</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p>时间序列，就是按时间顺序排列的，随时间变化的数据序列。我从自己实验平台上采的上位机和设备之间的通信包，按packets/s做数据就是时间序列。因为这个环境比较特殊，跟通信过程和应用层协议有关系，这里就不多说了，因为不是传统网络包，所以前期还是做了一些分析的，结果是这些数据是周期性的，但是不平稳，总之是可以做很多东西的。</p>
<p>随机过程的特征有均值、方差、协方差等。<br>如果随机过程的特征随着时间变化，则此过程是非平稳的；相反，如果随机过程的特征不随时间而变化，就称此过程是平稳的。非平稳时间序列分析时，若导致非平稳的原因是确定的，可以用的方法主要有趋势拟合模型、季节调整模型、移动平均、指数平滑等方法。<br>若导致非平稳的原因是随机的，方法主要有ARIMA（autoregressive integrated moving average）及自回归条件异方差模型等。</p>
<p>ARIMA，就是AR，I，MA的结合。ARIMA(p,d,q)模型，其中 d 是差分的阶数，用来得到平稳序列。</p>
<p>AR是自回归, p为相应的自回归项。MA为移动平均，q为相应的移动平均项数。ARIMA和ARMA的区别，就是公式左边的x变成差分算子，保证数据的稳定性。一般取两阶以下就好了。</p>
<p>贴公式：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-e3570dff8b96abda.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/515" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-994e75bd9e86802c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/517" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-9fefb203841154f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/617" alt=""></p>
<p><img src="https://upload-images.jianshu.io/upload_images/1667471-80aa172908765c87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/488" alt=""></p>
<h3 id="上手"><a href="#上手" class="headerlink" title="上手"></a>上手</h3><p>ARIMA模型运用的基本流程有几下几步：</p>
<pre><code>数据可视化，识别平稳性。
对非平稳的时间序列数据，做差分，得到平稳序列。
建立合适的模型。
平稳化处理后，若偏自相关函数是截尾的，而自相关函数是拖尾的，则建立AR模型；
若偏自相关函数是拖尾的，而自相关函数是截尾的，则建立MA模型；
若偏自相关函数和自相关函数均是拖尾的，则序列适合ARMA模型。
模型的阶数在确定之后，对ARMA模型进行参数估计，比较常用是最小二乘法进行参数估计。
假设检验，判断（诊断）残差序列是否为白噪声序列。
利用已通过检验的模型进行预测。
</code></pre><p>1.数据的平稳性处理<br>ARIMA模型建模时，首先采用ADF（Augmented Dickey-Fuller）单位根检验来判断数据的平稳性。通常可以画出时间序列的散点图或折线图，来对所研究的时间序列进行大致的平稳性判断。对非平稳的时间序列，一般取对数处理或进行差分处理，然后判断修正后的数据序列的平稳性。若采取差分的形式，此时进行差分的次数就是ARIMA(p,d,q)模型中的阶数d。在差分运算过程中，阶数并不是越大越好，差分运算的过程是信息加工提取的过程，因此，一般差分次数不超过2次。时间序列数据被平稳化处理后，ARIMA(p,d,q)模型就转化为ARMA(p,q)模型。</p>
<p>2.建立模型<br>通常在时间序列分析中，采用自相关函数（ACF）、偏自相关函数（PACF）来判别ARMA(p,q)模型的系数和阶数。自相关函数(ACF)描述时间序列观测值与其过去的观测值之间的线性相关性。偏自相关函数(PACF)描述在给定中间观测值的条件下时间序列观测值与其过去的观测值之间的线性相关性。<br><img src="http://img.blog.csdn.net/20150602093858686?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXNwaXJpbnZhZ3JhbnQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>这个表基本上就够用啦</p>
<p>3.参数估计<br>时间序列分析模型的阶数在确定之后，然后应当对ARMA模型进行参数估计。比较常用是最小二乘法进行参数估计，但是在所有的时间序列模型中，MA模型的参数估计相对比较困难，因此，尽量避免使用高阶的MA模型和ARMA模型。</p>
<p>4.模型验证<br>通过上述步骤后，应对通过模型取得的估计结果进行检验与诊断，以验证所选用的模型是否合适。这一过程主要检验所拟合的时间序列模型是否客观合理。针对模型的合理性检验，通常从两个方面进行判断：1、要验证所拟合的时间序列<br>模型的参数估计值是否有显著性；2、要验证所拟合的时间序列模型的残差序列是否是白噪声序列，即残差序列的独立性检验。残差序列可由估计出来的模型计算得到，如果残差序列的自相关函数不显著非零，可以认为是独立的。若这两项验证通过，则认为该模型是合理的，否则，应重新选取模型，上述步骤，选出有效的模型，然后应用该模型进行预测。</p>
<p>这里不把我对工控数据的arima分析贴出来了，其实比较一下仔细分析选的模型和autoarima的特性还是蛮有趣的</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2017/08/02/git账号/"><span>查看本地git的账号是否是目前登陆的邮箱</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/08/02/git账号/" rel="bookmark">
        <time class="entry-date published" datetime="2017-08-02T01:22:21.000Z">
          2017-08-02
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>我的GitHub账号很乱。感觉</p>
<p>我查了一下，很多人出现跟我类似的问题，都在问为什么自己明明有干嘛干嘛啊，为什么没有绿格子，基本上解决办法就是本地看一下git账号邮箱啊用户名啊是什么就能解决了。</p>
<p><code>$ git config --global user.name &quot;John Doe&quot;</code></p>
<p><code>$ git config --global user.email johndoe@example.com</code></p>
<p>这个是一开始的配置，可能大家配置的时候很不经意 就填了其他的。。像我这种有好几个邮箱的人很多吧。这个其实蛮关键的，因为以后你所有的项目都会默认使用这里配置的用户信息。</p>
<p>为什么没有绿格子呢，看一下配置信息就好啦</p>
<p><code>$ git config --list</code></p>
<p>不是自己登陆的邮箱账号的话就改吧</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2017/07/27/略谈EM算法/"><span>EM算法</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/07/27/略谈EM算法/" rel="bookmark">
        <time class="entry-date published" datetime="2017-07-27T01:22:21.000Z">
          2017-07-27
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="略谈EM算法"><a href="#略谈EM算法" class="headerlink" title="略谈EM算法"></a>略谈EM算法</h2><p>目前虽然没有用到这个的项目，但是毕竟是经典算法，而且跟概率相关的对于我而言都比较绕，就仔细琢磨琢磨啦。</p>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>先说最大似然估计好了，我觉得这个蛮好理解的：</p>
<p>通俗的说：现在已经拿到了很多个样本（你的数据集中所有因变量），这些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前面已经实现的样本值发生概率最大。这时是求样本所有观测的联合概率最大化，是个连乘积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数为零，就可以通过解方程（组），得到最大似然估计值。</p>
<p><img src="http://img.blog.csdn.net/20170528003838359" alt=""></p>
<p>实际中为了便于分析，定义了对数似然函数：</p>
<p><img src="http://img.blog.csdn.net/20170528003844453" alt=""></p>
<p><img src="http://img.blog.csdn.net/20170528003850144" alt=""></p>
<p>求最大似然估计量的一般步骤：</p>
<pre><code>（1）写出似然函数；

（2）对似然函数取对数，并整理；

（3）求导数；

（4）解似然方程。
</code></pre><p>最大似然估计的特点：</p>
<pre><code>1.比其他估计方法更加简单；

2.收敛性：无偏或者渐近无偏，当样本数目增加时，收敛性质会更好；

3.如果假设的类条件概率模型正确，则通常能获得较好的结果。但如果假设模型出现偏差，将导致非常差的估计结果。
</code></pre><h3 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h3><p>都说到极大似然了，继续讲一下贝叶斯决策吧。也有很多科普是先从贝叶斯决策引出极大似然的。</p>
<p><img src="http://img.blog.csdn.net/20170528002022807" alt=""></p>
<p>我们要求的是后验概率，就得先知道先验概率和似然（类条件概率）。这里先验概率估计较简单，1、每个样本所属的自然状态都是已知的（有监督学习）；2、依靠经验；3、用训练样本中各类出现的频率估计。类条件概率的估计（非常难），原因包括：概率密度函数包含了一个随机变量的全部信息；样本数据可能不多；特征向量x的维度可能很大等等。总之要直接估计类条件概率的密度函数很难。解决的办法就是，把估计完全未知的概率密度转化为估计参数。这里就将概率密度估计问题转化为参数估计问题，极大似然估计就是一种参数估计方法。</p>
<h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>M算法当做最大似然估计的拓展，解决难以给出解析解的最大似然估计（MLE）问题。EM算法引入了隐变量。什么是隐变量呢，比如你知道A是高斯分布，和一些样本，就能估计它的参数，B也是。但是如果A，B是混合的，就不太好估计，因为抽取得到的每个样本都不知道是从哪个分布抽取的，（直观上是这样，公式推导结果也是如此，出现了log在两个求和符号之间的情况）</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28X%3B%5Ctheta%29%3D%5Csum_%7Bk%3D1%7D%5EK%5Cpi_kN%28x%3B+%5Cmu_k%2C+%5Csigma_k%29%3D%5Csum_ZP%28Z%3B%5Cpi%29P%28X%7CZ%3B%5Cmu%2C%5Csigma%29" alt=""></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Ctheta%5E%2A+%26%3D+%5Carg%5Cmax_%5Ctheta+%5Csum_X+logP%28X%3B%5Ctheta%29+%5C%5C+%26%3D%5Carg%5Cmax_%5Ctheta+%5Csum_X+log+%5Csum_ZP%28Z%3B%5Cpi%29P%28X%7CZ%3B%5Cmu%2C%5Csigma%29+%5C%5C+%26%3D%5Carg%5Cmax_%5Ctheta+%5Csum_X+log+%5Csum_ZP%28X%2CZ%3B%5Ctheta%29+%5Cend%7Baligned%7D" alt=""></p>
<p>这时就需要EM算法。EM算法就是这样，假设我们想估计知道A和B两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。E步（求期望），第三步被称作M步（求极大化），于是EM算法就在不停的EM、EM、EM….，所以被叫做EM算法！使用EM的好处就是，分离不开，就不分离。用旧的变量来表达新的变量，一步步迭代，也能找到局部最优。其实也就坐标上升法，含有隐变量对数似然求导比较复杂，因此先固定参数求隐变量后验分布(E步)，然后固定隐变量求参数(M步)，交替进行。</p>
<p>举个非常棒的例子，看完基本上就能懂什么是EM算法，如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-a95770a0f41ed0873106d4a1f2dd6b7d_hd.jpg" alt=""></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2017/06/02/正则化的问题/"><span>L1正则化和L2正则化</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/06/02/正则化的问题/" rel="bookmark">
        <time class="entry-date published" datetime="2017-06-02T03:39:21.000Z">
          2017-06-02
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="正则化的问题"><a href="#正则化的问题" class="headerlink" title="正则化的问题"></a>正则化的问题</h2><p>先说一下范数是什么：<br><img src="https://upload-images.jianshu.io/upload_images/2027163-98307b25668cfa3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/599" alt=""></p>
<blockquote>
<p>0范数，向量中非零元素的个数。<br>1范数，为绝对值之和。<br>2范数，就是通常意义上的模，表示某个向量中所有元素平方和再开根， 也就是欧几里得距离公式。</p>
</blockquote>
<p>也就是L1正则化和L2正则化，或者L1范数和L2范数</p>
<p>L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。Lasso回归的损失函数，式中加号后面一项α||w||1，即为L1正则化项。Ridge回归的损失函数，式中加号后面一项α||w||2^2,即为L2正则化项。</p>
<p><img src="http://img.blog.csdn.net/20160904184228158" alt=""></p>
<p><img src="http://img.blog.csdn.net/20160904184314333" alt=""></p>
<p>它们的作用：</p>
<p>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择<br>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合</p>
<p>一般用l2正则化防止过拟合，那先讲它为什么能防止这个好了：</p>
<p>1.模型过于复杂是因为模型尝试去兼顾各个测试数据点， 导致模型函数如下图，处于一种动荡的状态， 每个点的到时在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。（很好理解吧，有的过拟合出来的曲线拐来拐去的，覆盖了每个点）如果发生过拟合， 参数θ一般是比较大的值， 加入惩罚项后， 只要控制λ的大小，当λ很大时，θ1到θn就会很小，即达到了约束数量庞大的特征的目的。</p>
<p>2.从贝叶斯的角度来分析， 正则化是为模型参数估计增加一个先验知识，先验知识会引导损失函数最小值过程朝着约束方向迭代。 L1正则是Laplace先验，L2是高斯先验。整个最优化问题可以看做是一个最大后验估计，其中正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计。（这一点有点难理解，需要假设参数先验分布之后推导）</p>
<p>然后是L1正则化。它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。</p>
<p><img src="http://img.blog.csdn.net/20160904184428459" alt=""></p>
<p>上图是L1正则化的表示，这里简单假设w是二维，所以画出的图是这样的，图中等值线是J0<br>的等值线，黑色方形是L<br>函数的图形。在图中，当J0<br>等值线与L<br>图形首次相交的地方就是最优解。上图中J0<br>与L<br>在L<br>的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是(w1,w2)=(0,w)<br>。可以直观想象，因为L<br>函数有很多<em>突出的角</em>（二维情况下四个，多维情况下更多），J0<br>与这些角接触的机率会远大于与L<br>其它部位接触的机率，而在这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2017/05/18/样本不平衡/"><span>机器学习中样本不平衡的解决办法</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/05/18/样本不平衡/" rel="bookmark">
        <time class="entry-date published" datetime="2017-05-18T15:35:21.000Z">
          2017-05-18
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>##机器学习中样本不均衡怎么办？</p>
<p>手头在做的工业数据异常检测，就有这样的问题。或者说很多异常检测方向的基于机器学习的检测算法都会面临这个问题，异常样本太少，不处理的话，很容易被当成异常数据不处理了，所以要有解决办法。</p>
<p>通过过抽样和欠抽样解决样本不均衡：</p>
<blockquote>
<p>抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。过抽样过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法。欠抽样欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。</p>
</blockquote>
<p>通过正负样本的惩罚权重解决样本不均衡:</p>
<blockquote>
<p>通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。使用这种方法时需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight: {dict, ‘balanced’}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y))。如果算法本身支持，这种思路是更加简单且高效的方法。</p>
</blockquote>
<p> 通过组合/集成方法解决样本不均衡组合/集成方法:</p>
<blockquote>
<p> 指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。</p>
</blockquote>
<p> 通过特征选择解决样本不均衡:</p>
<blockquote>
<p> 上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。提示 上述几种方法的思路都是基于分类问题解决的。实际上，这种从大规模数据中寻找罕见数据的情况，也可以使用非监督式的学习方法，例如使用One-class SVM进行异常检测。分类是监督式方法，前期是基于带有标签（Label）的数据进行分类预测；而采用非监督式方法，则是使用除了标签以外的其他特征进行模型拟合，这样也能得到异常数据记录。所以，要解决异常检测类的问题，先是考虑整体思路，然后再考虑方法模型。</p>
</blockquote>
<p>其实这个东西是有很多文章用OCSVM做的，说是SVM其实是聚类，或者说SVDD更好，目前没有用这个，以后试试效果比较一下跟平衡权重哪个更好。</p>
<p>☺️☺️☺️☺️☺️☺️☺️☺️☺️☺️☺️☺️☺️</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>



  <article>

  
    
    <h3 class="article-title"><a href="/2017/05/06/写写常见的几种最优化方法/"><span>梯度下降法和牛顿法</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2017/05/06/写写常见的几种最优化方法/" rel="bookmark">
        <time class="entry-date published" datetime="2017-05-06T08:35:00.000Z">
          2017-05-06
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="写写常见的几种最优化方法"><a href="#写写常见的几种最优化方法" class="headerlink" title="写写常见的几种最优化方法"></a>写写常见的几种最优化方法</h2><p>首先肯定是要说泰勒公式的了。。</p>
<p>泰勒公式是一个用函数在某点的信息描述其<em>附近</em>取值的公式。</p>
<p><strong>局部有效性</strong></p>
<p>它的基本形式是：<img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp5hmziysij31kw089tan.jpg" alt=""></p>
<p>这是后续优化方法的迭代基础</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>在机器学习任务中，需要最小化损失函数L（theta），其中theta是要求解的模型参数。梯度下降法常用来求解这种无约束最优化问题，它是一种迭代方法:选取初值，不断迭代，更新theta的值，进行损失函数的极小化。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。</p>
<p><img src="http://wx1.sinaimg.cn/mw690/006cxA6Hgy1fp5ht3pqwqj31kw0n8n2l.jpg" alt=""></p>
<p>这个图大概能表示梯度下降搜索迭代的概念</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Gradient_descent.png/350px-Gradient_descent.png" alt=""></p>
<p>同时还有共轭梯度下降，次梯度下降等。　共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p>
<p>还有两个概念：批量梯度下降法（Batch Gradient Descent，BGD），它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度会相当的慢。所以，这就引入了另外一种方法——随机梯度下降。随机梯度下降（Stochastic Gradient Descent，SGD）：随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>
<p>随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p>
<p>对批量梯度下降法和随机梯度下降法的总结：</p>
<p>   批量梯度下降—最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p>
<p>随机梯度下降—最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</p>
<p>所以引入了mini-batch 梯度下降的概念，很棒了。</p>
<h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>牛顿法和梯度下降法最直观的差别就是，前者进行了二阶的泰勒展开：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/006cxA6Hgy1fp5hxpzbrbj31kw0xnwn8.jpg" alt=""><br>一个动图<br><img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif" alt=""></p>
<p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p>
<p>但是！牛顿法每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</p>
<p>所以就有了拟牛顿法</p>
<h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><p>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
<p>这个又有很多内容了，DFP，BFGS啥的，在python里用起来很方便，但是最好还是要弄懂原理的，感觉心里会更清晰一点呢✌🏻</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>




<nav class="pagination">
  
  
  <a href="/page/2/" class="pagination-next">下一页</a>
  
</nav>
    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2018 Yang He
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>