<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>一些CNN知识 | 呜呜部落格(•̀ᴗ•́)و ̑̑</title>

  
  <meta name="author" content="Yang He">
  

  
  <meta name="description" content="在做的实验室工作是工控系统中的对抗机器学习问题，这是一个从计算机视觉中扩展的问题，所以会遇到很多涉及到CNN的知识，这篇博客总结一下CNN的基础知识。
基本结构卷积神经网络是一种层次模型，其输入是原始数据，如图像、原始音频数据等。卷积神经网络通过卷积操作、汇合操作和非线性激活函数映射等一系列操作的层">
  

  
  
  <meta name="keywords" content="">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="一些CNN知识"/>

  <meta property="og:site_name" content="呜呜部落格(•̀ᴗ•́)و ̑̑"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="呜呜部落格(•̀ᴗ•́)و ̑̑" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">呜呜部落格(•̀ᴗ•́)و ̑̑</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>一些CNN知识</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2018/03/19/CNN/" rel="bookmark">
        <time class="entry-date published" datetime="2018-03-19T15:35:21.000Z">
          2018-03-19
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p>在做的实验室工作是工控系统中的对抗机器学习问题，这是一个从计算机视觉中扩展的问题，所以会遇到很多涉及到CNN的知识，这篇博客总结一下CNN的基础知识。</p>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><p>卷积神经网络是一种层次模型，其输入是原始数据，如图像、原始音频数据等。卷积神经网络通过卷<br>积操作、汇合操作和非线性激活函数映射等一系列操作的层层堆叠，将高层语义信息逐层由原<br>始数据输入层中抽取出来，逐层抽象，这一过程便是“前馈运算”。其中，不同类型操作在卷积神经网络中一般称作“层”:卷积操作对应“卷积层”，汇合操作对应“汇合层”等等。最终，卷积神经网络的最后一层将其目标任务(分类、回归等)形式化为目标函数。通过计算预测值与真实值之间的误差或损失，凭借反向传播算法将误差或损失由最后一层逐层向前反馈，更新每层参数。</p>
<blockquote>
<p>简单来讲，它就是一个搭积木的过程，将不同的层作为一个基本操作搭建在原始数据上，并用损失函数的计算作为结束。<br>很自然会想到和RNN的区别，CNN是在一个深度上纵向学习的过程，层数不断堆叠而RNN是引入时间信息后横向的学习过程。</p>
<p>用通俗的话来理解，就像是用一层层“滤镜”不断扫原始的图像，发现特征，最后再用一个全连接层进行最后的分类或者用卷积层替代完成。</p>
</blockquote>
<p><img src="http://wx4.sinaimg.cn/mw690/006cxA6Hgy1fsvlhtrnsqj30r80nqqd2.jpg" alt=""></p>
<h2 id="基本部件"><a href="#基本部件" class="headerlink" title="基本部件"></a>基本部件</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积是一种局部操作，通过一定大小的卷积核作用于局部图像区域获得图像的局部信息。之前信号处理学过卷积，它的物理意义可以理解成一个函数（如：单位响应）在另一个函数（如：输入信号）上的加权叠加（离散情况下容易理解）。之所以把卷积概念引入到神经网络中，是因为相比于之前的全连接层，需要大量的参数，原因在于每个神经元都和相邻层的神经元相连接，但是这种连接方式对于CV是不完全需要的。全连接层的方式对于图像数据来说似乎显得不这么友好，因为图像本身具有“二维空间特征”，通俗点说就是局部特性。所以如果我们可以用某种方式对一张图片的某个典型特征识别，那么这张图片的类别也就知道了，即引入卷积的概念。也就是说神经网络不再是对每个像素的输入信息做处理了,而是图片上每一小块像素区域进行处理, 这种做法加强了图片信息的连续性. 使得神经网络能看到图形, 而非一个点. 这种做法同时也加深了神经网络对图片的理解。</p>
<p><img src="http://wx2.sinaimg.cn/small/006cxA6Hgy1fsvp2grk1ug30em0aojsv.gif" alt=""></p>
<p>具体来说, 卷积神经网络有一个批量过滤器，filter, 持续不断的在图片上滚动收集图片里的信息,每一次收集的时候都只是收集一小块像素区域, 然后把收集来的信息进行整理, 这时候整理出来的信息有了一些实际上的呈现, 比如这时的神经网络能看到一些边缘的图片信息, 然后在以同样的步骤, 用类似的批量过滤器（权值共享）扫过产生的这些边缘信息, 神经网络从这些边缘信息里面总结出更高层的信息结构,比如说总结的边缘能够画出眼睛,鼻子等等. 再经过一次过滤, 脸部的信息也从这些眼睛鼻子的信息中被总结出来. 最后我们再把这些信息套入几层普通的全连接神经层进行分类, 这样就能得到输入的图片能被分为哪一类的结果了.</p>
<p>同一层的神经元可以共享卷积核，那么对于高位数据的处理将会变得非常简单。并且使用卷积核后图片的尺寸变小，方便后续计算，并且我们不需要手动去选取特征，只用设计好卷积核的尺寸，数量和滑动的步长就可以让它自己去训练了，</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>在每一次卷积的时候, 神经层可能会无意地丢失一些信息. 这时, 池化 (pooling) 就可以很好地解决这一问题. 而且池化是一个筛选过滤的过程, 能将 layer 中有用的信息筛选出来, 给下一个层分析. 同时也减轻了神经网络的计算负担 。 也就是说在卷积的时候, 我们不压缩长宽, 尽量地保留更多信息, 压缩的工作就交给池化了,这样的一项附加工作能够很有效的提高准确性。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Max-Pooling: 选择Pooling窗口中的最大值作为采样值；</span><br><span class="line"></span><br><span class="line">Mean-Pooling: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值；</span><br></pre></td></tr></table></figure>
<p><img src="http://wx4.sinaimg.cn/small/006cxA6Hgy1fsvpv8hpimj31220lsju6.jpg" alt=""></p>
<p>汇合操作后的结果相比其输入降小了，其实汇合操 作实际上就是一种“降采样”操作</p>
<h3 id="激活函数和zero-padding"><a href="#激活函数和zero-padding" class="headerlink" title="激活函数和zero-padding"></a>激活函数和zero-padding</h3><p>激活函数层又称非线性映射层， 顾名思义，激活函数的引入为的是增加整个网络的表达能力(即非线性)。否则， 若干线性操作层的堆叠仍然只能起到线性映射的作用，无法形成复杂的函数。 在实际使用中，有多达十几种激活函数可供选择</p>
<p>所以到现在为止，我们的图片由4x4，通过卷积层变为3x3，再通过池化层变化2x2，如果我们再添加层，那么图片岂不是会越变越小？这个时候我们就会引出“Zero Padding”（补零），它可以帮助我们保证每次经过卷积或池化输出后图片的大小不变，如，上述例子我们如果加入Zero Padding，再采用3*3的卷积核，那么变换后的图片尺寸与原图片尺寸相同，如下图所示：</p>
<p><img src="http://wx2.sinaimg.cn/small/006cxA6Hgy1fsvq73zmn1j30pg0kcgms.jpg" alt=""></p>
<h3 id="总结一些中心思想"><a href="#总结一些中心思想" class="headerlink" title="总结一些中心思想"></a>总结一些中心思想</h3><ol>
<li>局部感受野：普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上；而在卷积神经网络中，每个隐层节点只连接到图像某个足够小局部的像素点上，从而大大减少需要训练的权值参数。比如1000×1000的图像，使用10×10的感受野，那么每个神经元只需要100个权值参数；不过由于需要将输入图像扫描一遍，共需要991×991个神经元（参数数目减少了一个数量级，不过还是太多）。</li>
<li>权值共享：在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。继续前面的例子，虽然需要991×991个神经元，但是它们的权值是共享的，所以只需要100个权值参数，以及1个偏置参数。作为补充，在CNN中的每个隐藏，一般会有多个卷积核。</li>
<li>池化：在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。以最大池化（Max Pooling）为例，1000×1000的图像经过10×10的卷积核卷积后，得到的是991×991的特征图，然后使用2×2的池化规模，即每4个点组成的小方块中，取最大的一个作为输出，最终得到的是496×496大小的特征图。</li>
</ol>
<blockquote>
<p>训练过程也是前向计算和反向传播</p>
<p>一些问题：卷积核的参数如何确定？随机初始化一个数值后，是如何训练得到一个能够识别某些特征的卷积核的？如何调整CNN里的参数？如何设计最适合的CNN网络结构？对于这些问题，看完网上资料和文献后有了基本的理解。</p>
</blockquote>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    

    

    </div>

    
  </div>
</article>


    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2018 Yang He
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>